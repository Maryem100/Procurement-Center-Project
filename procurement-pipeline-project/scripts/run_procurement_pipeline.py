"""
Orchestration du pipeline - Transfert HDFS corrigÃ©
Utilise webhdfs ou copy direct sans docker
"""

import subprocess
import sys
from datetime import datetime
from pathlib import Path
import os

class ProcurementPipeline:
    
    def __init__(self):
        self.start_time = datetime.now()
        self.steps_completed = 0
        self.total_steps = 6
        # Configuration HDFS
        self.hdfs_host = os.getenv('HDFS_NAMENODE', 'namenode')
        self.hdfs_port = os.getenv('HDFS_PORT', '9000')
        
    def print_header(self):
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          PIPELINE DE PROCUREMENT - EXÃ‰CUTION COMPLÃˆTE        â•‘
â•‘          Date: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}                         â•‘
â•‘          HDFS: {self.hdfs_host}:{self.hdfs_port}
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
    
    def print_step(self, step_num, description):
        print(f"\n{'='*70}")
        print(f"  Ã‰TAPE {step_num}/{self.total_steps}: {description}")
        print(f"{'='*70}")
    
    def run_python_script(self, script_path):
        """ExÃ©cute un script Python"""
        try:
            result = subprocess.run(
                [sys.executable, script_path],
                capture_output=True,
                text=True,
                check=True
            )
            print(result.stdout)
            if result.stderr:
                print(f"âš ï¸  Warnings: {result.stderr}")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ ERREUR: {e.stderr}")
            return False
    
    def transfer_to_hdfs_subprocess(self, local_path, hdfs_path, description):
        """TransfÃ¨re via subprocess en exÃ©cutant hadoop directement"""
        print(f"\nğŸ“¤ Transfert vers HDFS: {description}")
        
        local_path = Path(local_path)
        if not local_path.exists():
            print(f"âš ï¸  Chemin local inexistant: {local_path}")
            return False
        
        try:
            # 1. CrÃ©er le rÃ©pertoire HDFS
            cmd_mkdir = [
                'hadoop', 'fs', '-mkdir', '-p', hdfs_path
            ]
            result = subprocess.run(cmd_mkdir, capture_output=True, text=True)
            if result.returncode != 0:
                print(f"âš ï¸  Erreur mkdir HDFS: {result.stderr}")
            else:
                print(f"   âœ“ RÃ©pertoire crÃ©Ã©: {hdfs_path}")
            
            # 2. TransfÃ©rer les fichiers
            if local_path.is_dir():
                # Copier tous les fichiers du rÃ©pertoire
                for file in local_path.glob('*'):
                    if file.is_file():
                        cmd_put = [
                            'hadoop', 'fs', '-put', '-f',
                            str(file), f"{hdfs_path}/"
                        ]
                        result = subprocess.run(cmd_put, capture_output=True, text=True)
                        if result.returncode == 0:
                            print(f"   âœ“ {file.name} transfÃ©rÃ©")
                        else:
                            print(f"   âŒ Erreur transfert {file.name}: {result.stderr}")
            else:
                # Copier un seul fichier
                cmd_put = [
                    'hadoop', 'fs', '-put', '-f',
                    str(local_path), hdfs_path
                ]
                result = subprocess.run(cmd_put, capture_output=True, text=True)
                if result.returncode == 0:
                    print(f"   âœ“ Fichier transfÃ©rÃ©: {local_path.name}")
                else:
                    print(f"   âŒ Erreur transfert: {result.stderr}")
                    return False
            
            return True
            
        except Exception as e:
            print(f"âŒ Erreur: {e}")
            return False
    
    def verify_hdfs_content(self, hdfs_path):
        """VÃ©rifie le contenu HDFS"""
        try:
            cmd = ['hadoop', 'fs', '-ls', hdfs_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                file_count = len([l for l in lines if l and not l.startswith('total')])
                print(f"   âœ“ VÃ©rification HDFS: {file_count} Ã©lÃ©ments dans {hdfs_path}")
                return True
            else:
                print(f"   âš ï¸  Impossible de vÃ©rifier: {result.stderr}")
                return False
        except Exception as e:
            print(f"   âš ï¸  Erreur vÃ©rification: {e}")
            return False
    
    def run(self):
        """ExÃ©cute le pipeline complet"""
        self.print_header()
        
        # Ã‰TAPE 1: AgrÃ©gation des commandes
        self.print_step(1, "AgrÃ©gation des commandes clients")
        if self.run_python_script("scripts/transformation/aggregate_orders.py"):
            self.steps_completed += 1
            
            # Transfert vers HDFS
            if self.transfer_to_hdfs_subprocess(
                "data/processed/aggregated_orders",
                "/procurement/processed/aggregated_orders",
                "Commandes agrÃ©gÃ©es"
            ):
                self.verify_hdfs_content("/procurement/processed/aggregated_orders")
        
        # Ã‰TAPE 2: Calcul du net demand
        self.print_step(2, "Calcul du net demand")
        if self.run_python_script("scripts/transformation/calculate_net_demand.py"):
            self.steps_completed += 1
            
            # Transfert vers HDFS
            if self.transfer_to_hdfs_subprocess(
                "data/processed/net_demand",
                "/procurement/processed/net_demand",
                "Net demand"
            ):
                self.verify_hdfs_content("/procurement/processed/net_demand")
        
        # Ã‰TAPE 3: GÃ©nÃ©ration des commandes fournisseurs
        self.print_step(3, "GÃ©nÃ©ration des commandes fournisseurs")
        if self.run_python_script("scripts/transformation/generate_supplier_orders.py"):
            self.steps_completed += 1
            
            # Transfert vers HDFS
            if self.transfer_to_hdfs_subprocess(
                "data/output/supplier_orders",
                "/procurement/output/supplier_orders",
                "Commandes fournisseurs"
            ):
                self.verify_hdfs_content("/procurement/output/supplier_orders")
        
        # Ã‰TAPE 4: VÃ©rification finale HDFS
        self.print_step(4, "VÃ©rification de l'architecture HDFS complÃ¨te")
        try:
            cmd = ['hadoop', 'fs', '-du', '-h', '/procurement']
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                print(result.stdout)
                self.steps_completed += 1
        except Exception as e:
            print(f"âš ï¸  Erreur vÃ©rification: {e}")
        
        # Ã‰TAPE 5: RÃ©sumÃ© des fichiers gÃ©nÃ©rÃ©s
        self.print_step(5, "RÃ©sumÃ© des fichiers gÃ©nÃ©rÃ©s")
        self.print_summary()
        self.steps_completed += 1
        
        # Ã‰TAPE 6: Statistiques finales
        self.print_step(6, "Statistiques du pipeline")
        self.print_final_stats()
        self.steps_completed += 1
        
        # RÃ©sumÃ© final
        self.print_final_summary()
    
    def print_summary(self):
        """Affiche un rÃ©sumÃ© des fichiers gÃ©nÃ©rÃ©s"""
        
        # Compter les fichiers locaux
        agg_path = Path("data/processed/aggregated_orders")
        demand_path = Path("data/processed/net_demand")
        orders_path = Path("data/output/supplier_orders")
        
        agg_files = len(list(agg_path.glob("*.csv"))) if agg_path.exists() else 0
        demand_files = len(list(demand_path.glob("*.csv"))) if demand_path.exists() else 0
        order_files = len(list(orders_path.glob("*.json"))) if orders_path.exists() else 0
        
        print(f"""
ğŸ“Š Fichiers gÃ©nÃ©rÃ©s:
   â€¢ Commandes agrÃ©gÃ©es: {agg_files} fichiers
   â€¢ Net demand: {demand_files} fichiers
   â€¢ Commandes fournisseurs: {order_files} fichiers JSON
        """)
    
    def print_final_stats(self):
        """Affiche les statistiques finales"""
        
        net_demand_path = Path("data/processed/net_demand")
        net_demand_files = sorted(net_demand_path.glob("*.csv")) if net_demand_path.exists() else []
        
        total_skus = 0
        total_quantity = 0
        
        try:
            import pandas as pd
            for f in net_demand_files:
                df = pd.read_csv(f)
                if len(df) > 0:
                    total_skus += len(df)
                    if 'order_quantity' in df.columns:
                        total_quantity += df['order_quantity'].sum()
        except Exception as e:
            print(f"âš ï¸  Erreur lecture stats: {e}")
        
        print(f"""
ğŸ“ˆ Statistiques du pipeline:
   â€¢ Total SKUs commandÃ©s: {total_skus}
   â€¢ Total unitÃ©s commandÃ©es: {int(total_quantity)}
   â€¢ Dates traitÃ©es: {len(net_demand_files)}
        """)
    
    def print_final_summary(self):
        """Affiche le rÃ©sumÃ© final"""
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    RÃ‰SUMÃ‰ FINAL                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  âœ… Ã‰tapes complÃ©tÃ©es: {self.steps_completed}/{self.total_steps}                                â•‘
â•‘  â±ï¸  DurÃ©e totale: {duration:.2f} secondes                            â•‘
â•‘  ğŸ“… Date de fin: {end_time.strftime('%Y-%m-%d %H:%M:%S')}                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“ DonnÃ©es stockÃ©es dans:                                   â•‘
â•‘     â€¢ Local: data/processed/ et data/output/                 â•‘
â•‘     â€¢ HDFS: /procurement/processed/ et /procurement/output/  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        
        if self.steps_completed == self.total_steps:
            print("\nğŸ‰ PIPELINE EXÃ‰CUTÃ‰ AVEC SUCCÃˆS ! ğŸ‰\n")
        else:
            print(f"\nâš ï¸  Pipeline complÃ©tÃ© avec {self.total_steps - self.steps_completed} erreur(s)\n")

if __name__ == "__main__":
    pipeline = ProcurementPipeline()
    try:
        pipeline.run()
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ERREUR FATALE: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)